{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGnMvWUwnvgD"
      },
      "source": [
        "# AMLD 2022 Forecasting & Meta Learning Workshop\n",
        "\n",
        "In this notebook, we will get our hands dirty with Darts. We will do the following things:\n",
        "\n",
        "* **Part 1:** Forecasting passenger counts series for 300 airlines (`air` dataset). We will train one model per series.\n",
        "* **Part 2:** Using \"global\" models - i.e., models trained on all 300 series simultaneously. Here we split every timeseries into data from the trainset and data from the testset.\n",
        "* **Part 3:** We will try some *meta learning*, and see what happens if we train some global models on one (big) dataset (`m4` dataset) and use them on another dataset. Compared to part 2, m4 is the trainset and m3 will be our testset.\n",
        "* **Part 4:** We will reuse our pre-trained model(s) of Part 3 on another new dataset (`m3` dataset) and see how it compares to models specifically trained on this dataset.\n",
        "\n",
        "## Part 0: Setup (No code to write - execute only)\n",
        "First, we need to install the right libraries and make the right imports. For the deep learning models, it will help to use a GPU runtime. To get a GPU instance, click on the \"RAM/Disk\" info bars on the upper right, select \"Change runtime type\" and choose a GPU as hardware accelerator. The following command will show you the GPU available (if any). If there's no GPU available, you can still go ahead and work on CPU."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can run this command to see if there's a GPU:\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "7pP-5QufS7hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JXsclCdygxH"
      },
      "outputs": [],
      "source": [
        "!pip install darts &> /dev/null\n",
        "!pip install pyyaml==5.4.1 &> /dev/null\n",
        "!pip install xlrd==2.0.1 &> /dev/null\n",
        "!pip install matplotlib==3.1.3 &> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't be afraid, we will uncover what these imports mean through the workshop :)"
      ],
      "metadata": {
        "id": "kkdwZGPA8E2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filter out unecessary warnings during import\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "oavXs1eISrW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ha0yFbPzGt_"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import tqdm.notebook as tq\n",
        "from pytorch_lightning.callbacks import Callback, EarlyStopping\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
        "from torch import nn\n",
        "\n",
        "from darts import TimeSeries\n",
        "from darts.dataprocessing.transformers import Scaler\n",
        "from darts.metrics import mape, mase, smape\n",
        "from darts.models import *\n",
        "from darts.utils.data import HorizonBasedDataset\n",
        "from darts.utils.losses import SmapeLoss\n",
        "from darts.utils.utils import ModelMode, SeasonalityMode, TrendMode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUUcOuC43zF7"
      },
      "source": [
        "We define the forecast horizon here - for all of the (monthly) time series used in this notebook, we'll be interested in forecasting 18 months in advance. We pick 18 months as this is what is used in the M3/M4 competitions for monthly series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPpceDS837z2"
      },
      "outputs": [],
      "source": [
        "HORIZON = 18"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Datasets loading methods\n",
        "Here, we define some helper methods to load the three datasets we'll be playing with: `air`, `m3` and `m4`. \n",
        "\n",
        "First, we download the datasets (Note: we processed some of the datasets as pickle files for simplicity and speed):"
      ],
      "metadata": {
        "id": "p3L8CEK-ZXV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute this cell once to download all three datasets\n",
        "!curl -L https://github.com/unit8co/amld2022-forecasting-and-metalearning/blob/main/data/m3_dataset.xls\\?raw\\=true -o m3_dataset.xls\n",
        "!curl -L https://github.com/unit8co/amld2022-forecasting-and-metalearning/blob/main/data/passengers.pkl\\?raw\\=true -o passengers.pkl\n",
        "!curl -L https://github.com/unit8co/amld2022-forecasting-and-metalearning/blob/main/data/m4_monthly_scaled.pkl\\?raw\\=true -o m4_monthly_scaled.pkl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSzWi6hwbuQ7",
        "outputId": "2108415f-ba10-4381-c207-6575cd7c4eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   159  100   159    0     0    819      0 --:--:-- --:--:-- --:--:--   819\n",
            "100   170  100   170    0     0    258      0 --:--:-- --:--:-- --:--:--  166k\n",
            "100 1716k  100 1716k    0     0  1753k      0 --:--:-- --:--:-- --:--:-- 1753k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   159  100   159    0     0    815      0 --:--:-- --:--:-- --:--:--   811\n",
            "100   178  100   178    0     0    635      0 --:--:-- --:--:-- --:--:--   635\n",
            "100 1100k  100 1100k    0     0  1534k      0 --:--:-- --:--:-- --:--:-- 1534k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   166  100   166    0     0    917      0 --:--:-- --:--:-- --:--:--   917\n",
            "100   185  100   185    0     0    649      0 --:--:-- --:--:-- --:--:--   649\n",
            "100  270M  100  270M    0     0  43.9M      0  0:00:06  0:00:06 --:--:-- 58.0M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the methods below return two list of `TimeSeries`: one list of training series and one list of \"test\" series (of length `HORIZON`).\n",
        "\n",
        "For convenience, all the series are already scaled here, by multiplying each of them by a constant so that the largest value is 1. Such scaling is necessary for many models to work correctly (esp. deep learning models). It does not affect the sMAPE values, so we can evaluate the accuracy of our algorithms on the scaled series. In a real application, we would have to keep the Darts `Scaler` objects somewhere in order to inverse-scale the forecasts."
      ],
      "metadata": {
        "id": "liUVSe9DMpuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_m3() -> Tuple[List[TimeSeries], List[TimeSeries]]:\n",
        "    print('building M3 TimeSeries...')\n",
        "\n",
        "    # Read DataFrame\n",
        "    df_m3 = (pd.read_excel('m3_dataset.xls', 'M3Month'))\n",
        "\n",
        "    # Build TimeSeries\n",
        "    m3_series = []\n",
        "    for row in tq.tqdm(df_m3.iterrows(), position=0, leave=True):\n",
        "        s = row[1]\n",
        "        start_year = int(s['Starting Year'])\n",
        "        start_month = int(s['Starting Month'])\n",
        "        values_series = s[6:].dropna()\n",
        "        if start_month == 0:\n",
        "            continue\n",
        "        \n",
        "        start_date = datetime(year=start_year, month=start_month, day=1)\n",
        "        time_axis = pd.date_range(start_date, periods=len(values_series), freq='M')\n",
        "        series = TimeSeries.from_times_and_values(time_axis, values_series.values).astype(np.float32)\n",
        "        m3_series.append(series)\n",
        "\n",
        "    print('\\nThere are {} monthly series in the M3 dataset'.format(len(m3_series)))\n",
        "\n",
        "    # Split train/test\n",
        "    print('splitting train/test...')\n",
        "    m3_train = [s[:-HORIZON] for s in m3_series]\n",
        "    m3_test = [s[-HORIZON:] for s in m3_series]\n",
        "\n",
        "    # Scale so that the largest value is 1\n",
        "    print('scaling...')\n",
        "    scaler_m3 = Scaler(scaler=MaxAbsScaler())\n",
        "    m3_train_scaled: List[TimeSeries] = scaler_m3.fit_transform(m3_train)\n",
        "    m3_test_scaled: List[TimeSeries] = scaler_m3.transform(m3_test)\n",
        "\n",
        "    print('done. There are {} series, with average training length {}'.format(\n",
        "        len(m3_train_scaled), np.mean([len(s) for s in m3_train_scaled])\n",
        "    ))\n",
        "    return m3_train_scaled, m3_test_scaled\n",
        "\n",
        "def load_air() -> Tuple[List[TimeSeries], List[TimeSeries]]:\n",
        "    # load TimeSeries\n",
        "    print('loading air TimeSeries...')\n",
        "    with open('passengers.pkl', 'rb') as f:\n",
        "        all_air_series = pickle.load(f)\n",
        "\n",
        "    # Split train/test\n",
        "    print('splitting train/test...')\n",
        "    air_train = [s[:-HORIZON] for s in all_air_series]\n",
        "    air_test = [s[-HORIZON:] for s in all_air_series]\n",
        "\n",
        "    # Scale so that the largest value is 1\n",
        "    print('scaling series...')\n",
        "    scaler_air = Scaler(scaler=MaxAbsScaler())\n",
        "    air_train_scaled: List[TimeSeries] = scaler_air.fit_transform(air_train)\n",
        "    air_test_scaled: List[TimeSeries] = scaler_air.transform(air_test)\n",
        "\n",
        "    print('done. There are {} series, with average training length {}'.format(\n",
        "        len(air_train_scaled), np.mean([len(s) for s in air_train_scaled])\n",
        "    ))\n",
        "    return air_train_scaled, air_test_scaled\n",
        "\n",
        "def load_m4() -> Tuple[List[TimeSeries], List[TimeSeries]]:\n",
        "    # load TimeSeries - the splitting and scaling has already been done\n",
        "    print('loading M4 TimeSeries...')\n",
        "    with open('m4_monthly_scaled.pkl', 'rb') as f:\n",
        "        m4_series = pickle.load(f)\n",
        "    m4_train_scaled, m4_test_scaled = zip(*m4_series)\n",
        "\n",
        "    print('done. There are {} series, with average training length {}'.format(\n",
        "        len(m4_train_scaled), np.mean([len(s) for s in m4_train_scaled])\n",
        "    ))\n",
        "    return m4_train_scaled, m4_test_scaled"
      ],
      "metadata": {
        "id": "DNWjOs-_Z0zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we define a handy function to tell us how good a bunch of forecasted series are:"
      ],
      "metadata": {
        "id": "co-uraewuSmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_forecasts(pred_series: List[TimeSeries], \n",
        "                   test_series: List[TimeSeries]) -> List[float]:\n",
        "  \n",
        "    print('computing sMAPEs...')\n",
        "    smapes = smape(test_series, pred_series)\n",
        "    mean, std = np.mean(smapes), np.std(smapes)\n",
        "    print('Avg sMAPE: %.3f +- %.3f' % (mean, std))\n",
        "    plt.figure(figsize=(4,4), dpi=144)\n",
        "    plt.hist(smapes, bins=50)\n",
        "    plt.ylabel('Count')\n",
        "    plt.xlabel('sMAPE')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    return smapes"
      ],
      "metadata": {
        "id": "TzVPxA0fuZn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Local models on the `air` dataset\n",
        "\n",
        "### Preparing Data\n",
        "\n",
        "The `air` dataset shows the number of air passengers that flew in or out of the USA per carrier (or airline company) from the year 2000 until 2019.\n",
        "\n",
        "**Your turn:** First, you can load the train and test series by calling `load_air()` function that we have defined above."
      ],
      "metadata": {
        "id": "T6VEn8bJcu1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "air_train, air_test = ..."
      ],
      "metadata": {
        "id": "iD4zKE99dWxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgy20GU_6xjn"
      },
      "source": [
        "It's a good idea to start by visualising a few of the series to get a sense of what they look like. We can plot a series by calling `series.plot()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuoYeS5E5QJw"
      },
      "outputs": [],
      "source": [
        "for i in [1, 20, 50, 100, 250]:  # Feel free to plot a few other series\n",
        "    plt.figure(figsize=(4,4), dpi=144)\n",
        "    air_train[i].plot()\n",
        "    plt.ylabel('Passengers')\n",
        "    plt.xlabel('Time')\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTw4PlKf60Cw"
      },
      "source": [
        "We can see that most series look quite different, and they even have different time axes. \n",
        "\n",
        "Question: What is the shortest series available?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw0ZLbcQ5lCi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A useful function to evaluate models\n",
        "\n",
        "Below, we write a small function that will make our life easier for quickly trying and comparing different local models. We loop through each serie, fit a model and then evaluate on our test dataset. \n",
        "\n",
        "> ⚠️  Please note `tq.tqdm` is optional and is only there to help display the training progress (as you will see it can take some time when training 300+ time series)\n"
      ],
      "metadata": {
        "id": "Vtt4M98dg9e9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_local_model(train_series: List[TimeSeries], \n",
        "                     test_series: List[TimeSeries], \n",
        "                     model_cls, \n",
        "                     **kwargs) -> Tuple[List[float], float]:\n",
        "    preds = []\n",
        "    start_time = time.time()\n",
        "    for series in tq.tqdm(train_series):\n",
        "        model = model_cls(**kwargs)\n",
        "        model.fit(series)\n",
        "        pred = model.predict(n=HORIZON)\n",
        "        preds.append(pred)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    \n",
        "    smapes = eval_forecasts(preds, test_series)\n",
        "    return smapes, elapsed_time"
      ],
      "metadata": {
        "id": "p9HLgVQkg36Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building and evaluating models\n",
        "\n",
        "We can now try a first forecasting model on this dataset. As a first step, it is usually a good practice to see how a (very) naive model blindly repeating the last value of the training series performs. This can be done in Darts using a [NaiveSeasonal](https://unit8co.github.io/darts/generated_api/darts.models.forecasting.baselines.html#darts.models.forecasting.baselines.NaiveSeasonal) model:"
      ],
      "metadata": {
        "id": "QUTMqfh1ht3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naive_seasonal_last_smapes, naive_seasonal_last_elapsed_time  = eval_local_model(air_train, air_test, NaiveSeasonal, K=1)"
      ],
      "metadata": {
        "id": "pIo0vIK-g4FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the most naive model gives us a sMAPE of about 39.38.\n",
        "\n",
        "**Your turn:** Can we do better with a \"less naive\" model exploiting the fact that most monthly series have a seasonality of 12?"
      ],
      "metadata": {
        "id": "6RCqDCdwjbWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y9Y9rI_mg4LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All of the Darts forecasting models can be trained and used in the same way!\n",
        "So we invite you to go over the [list of models in the API documentation](https://unit8co.github.io/darts/generated_api/darts.models.forecasting.html) and try a few more models. Here are some suggestions:\n",
        "\n",
        "* `ExponentialSmoothing`\n",
        "* `Theta`\n",
        "* `ARIMA` - but the default parameters probably won't do very well. Using `p=12`, `d=1`, `q=0` might be a good start.\n",
        "* ... Your ideas here!\n",
        "\n",
        "We recommend that you keep track of the SMAPEs and elapsed times for each model. Later we will use these values for quickly comparing models. Some models will take longer than others to run. Don't hesitate to interrupt the execution or run only on a subset of series.\n",
        "\n",
        "**Your turn:** Try to get the lowest possible errors with some other models of your choice."
      ],
      "metadata": {
        "id": "Zym9JP6-kSti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_X_smapes, model_X_elapsed_time = eval_local_model(air_train, air_test, ModelX, **hyper_params_for_model_X)"
      ],
      "metadata": {
        "id": "b0dK_b61g4OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing models\n",
        "\n",
        "Below, we define a couple of functions that we will use to obtain an overview of the SMAPEs and time required to obtain the predictions."
      ],
      "metadata": {
        "id": "OFRSVc9pPCJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def smapes_boxplot(method_to_smapes: Dict[str, List[float]], title: str):\n",
        "  method_names = []\n",
        "  smapes = []\n",
        "  for curr_method_name, curr_smapes in method_to_smapes.items():\n",
        "    method_names += [curr_method_name] * len(curr_smapes)\n",
        "    smapes += curr_smapes\n",
        "  smapes_df = pd.DataFrame({'Method': method_names, 'sMAPE': smapes})\n",
        "  plt.figure(figsize=(7,4), dpi=144)\n",
        "  ax = sns.boxplot(x=\"Method\", y=\"sMAPE\", data=smapes_df)\n",
        "  ax.grid(False)\n",
        "  # Display median score on each box\n",
        "  medians = smapes_df.groupby(['Method'])['sMAPE'].median().round(decimals=2)\n",
        "  vertical_offset = smapes_df['sMAPE'].median() * 0.1\n",
        "  for xtick, name in enumerate(method_to_smapes.keys()):\n",
        "    ax.text(xtick, medians[name] + vertical_offset, medians[name], \n",
        "                  horizontalalignment='center', size='x-small', color='w', weight='semibold')\n",
        "  plt.xticks(rotation=90) \n",
        "  plt.title(title) \n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "def elapsed_time_barplot(method_to_elapsed_times: Dict[str, float], title: str):\n",
        "  elapsed_times_df = pd.DataFrame({'Method': method_to_elapsed_times.keys(), 'Elapsed time [s]': method_to_elapsed_times.values()})\n",
        "  ax = plt.figure(figsize=(7,4), dpi=144)\n",
        "  sns.barplot(x=\"Method\", y=\"Elapsed time [s]\", data=elapsed_times_df)\n",
        "  plt.xticks(rotation=90) \n",
        "  plt.title(title) \n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "FBy5eqrpFhZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn:** We are now ready to visualise our models. Fill in the cells below to call `smapes_boxplot()` and `elpased_time_barplot()` with the right arguments."
      ],
      "metadata": {
        "id": "3pBzARlMQQ5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smapes = {\n",
        "    'naive-last': naive_seasonal_last_smapes,\n",
        "    # 'model_X': model_X_smapes,\n",
        "    # ...\n",
        "}\n",
        "\n",
        "smapes_boxplot(smapes, title='sMAPEs on air passengers dataset')"
      ],
      "metadata": {
        "id": "AgnHS7mf-_0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elapsed_times = {\n",
        "    'naive-last': naive_seasonal_last_elapsed_time,    \n",
        "    # 'model_X': model_X_elapsed_time,\n",
        "    # ...\n",
        "}\n",
        "\n",
        "elapsed_time_barplot(elapsed_times, title='Predict durations on air passengers dataset')"
      ],
      "metadata": {
        "id": "Olnp0bVJFecR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also try to directly predict some of the forecasts in order to visualise them.\n",
        "\n",
        "What are your conclusions so far?\n",
        "\n",
        "What are your best forecasts? Let us know!\n",
        "\n",
        "## Part 2: Global models on the `air` dataset\n",
        "In this section we will use \"global models\" - that is, models that fit on multiple series at once. Darts has essentially two kinds of global models:\n",
        "* `RegressionModels` which are wrappers around sklearn-like regression models (Part 2.1).\n",
        "* PyTorch-based models, which offer various deep learning models (Part 2.2).\n",
        "\n",
        "Both models can be trained on multiple series by \"tabularizing\" the data - i.e., taking many (input, output) sub-slices from all the training series, and training machine learning models in a supervised fashion to predict the output based on the input.\n",
        "\n",
        "**Your turn** We will start by defining a function `eval_global_model()` which works similarly to `eval_local_model()`, but on global models. You can complete it below (hint: you will not need the for-loop that was present in `eval_local_model()`)."
      ],
      "metadata": {
        "id": "nS4NuMjdrjRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_global_model(train_series: List[TimeSeries], \n",
        "                      test_series: List[TimeSeries], \n",
        "                      model_cls, \n",
        "                      **kwargs) -> Tuple[List[float], float]:\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    model = ... # build your model here\n",
        "\n",
        "    ... # fit your model here\n",
        "\n",
        "    ... # get some predictions here\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    \n",
        "    smapes = eval_forecasts(preds, test_series)\n",
        "    return smapes, elapsed_time"
      ],
      "metadata": {
        "id": "ApbPFYAHc5Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2.1: Using Darts `RegressionModel`s.\n",
        "`RegressionModel` in Darts are forecasting models that can wrap around any \"scikit-learn compatible\" regression model to obtain forecasts. Compared to deep learning, they represent good \"go-to\" global models because they typically don't have many hyper-parameters and can be faster to train. In addition, Darts also offers some \"pre-packaged\" regression models such as `LinearRegressionModel` and `LightGBMModel`.\n",
        "\n",
        "We'll now use our function `eval_global_models()`. In the following cells, you can try using some regression models, for example:\n",
        "* `LinearRegressionModel`\n",
        "* `LightGBMModel`\n",
        "* `RegressionModel`(your_sklearn_model)\n",
        "\n",
        "You can refer to [the API doc](https://unit8co.github.io/darts/generated_api/darts.models.forecasting.regression_model.html) for how to use them.\n",
        "\n",
        "Important parameters are `lags` and `output_chunk_length`. They determine respectively the length of the lookback and \"lookforward\" windows used by the model, and they correspond to the lengths of the input/output subslices used for training. For instance `lags=24` and `output_chunk_length=12` mean that the model will consume the past 24 lags in order to predict the next 12. In our case, because the shortest training series has length 36, we must have `lags + output_chunk_length <= 36`. (Note that `lags` can also be a list of integers representing the individual lags to be consumed by the model instead of the window length)."
      ],
      "metadata": {
        "id": "y1hYJUyH7lAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_X_smapes, model_X_elapsed_time = eval_global_model(air_train, air_test, GlobalModelX, **hyper_params_for_model_X)"
      ],
      "metadata": {
        "id": "STjjdT6QraDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2.2: Using deep learning\n",
        "Below, we will train an N-BEATS model on our `air` dataset. Again, you can refer to [the API doc](https://unit8co.github.io/darts/generated_api/darts.models.forecasting.nbeats.html) for documentation on the hyper-parameters.\n",
        "The following hyper-parameters should be a good starting point, and training should take in the order of a minute or two.\n",
        "\n",
        "During training, you can have a look at the [N-BEATS paper](https://arxiv.org/abs/1905.10437)."
      ],
      "metadata": {
        "id": "zMxzNto5spdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Possible N-BEATS hyper-parameters\n",
        "\n",
        "# Slicing hyper-params:\n",
        "IN_LEN = 24\n",
        "OUT_LEN = 12\n",
        "\n",
        "# Architecture hyper-params:\n",
        "NUM_STACKS = 18\n",
        "NUM_BLOCKS = 3\n",
        "NUM_LAYERS = 3\n",
        "LAYER_WIDTH = 180\n",
        "COEFFS_DIM = 6\n",
        "LOSS_FN = SmapeLoss()\n",
        "\n",
        "# Training settings:\n",
        "LR = 5e-4\n",
        "BATCH_SIZE = 1024\n",
        "NUM_EPOCHS = 4"
      ],
      "metadata": {
        "id": "-ZDirbwpsoF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now complete the skeleton below to build, train and predict using an N-BEATS model:"
      ],
      "metadata": {
        "id": "Z6qVsp2yfUB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "## Use this to specify \"optimizer_kwargs\" parameter of the N-BEATS model:\n",
        "optimizer_kwargs={'lr': LR},\n",
        "\n",
        "## In addition, when using a GPU, you should specify this for \n",
        "## the \"pl_trainer_kwargs\" parameter of the N-BEATS model:\n",
        "pl_trainer_kwargs={\"enable_progress_bar\": True, \n",
        "                   \"accelerator\": \"gpu\",\n",
        "                   \"gpus\": -1,\n",
        "                   \"auto_select_gpus\": True}\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "nbeats_model_air = ... # Build the N-BEATS model here\n",
        "\n",
        "nbeats_model_air.fit(..., # fill in series to train on\n",
        "                     ...) # fill in number of epochs\n",
        "\n",
        "# get predictions\n",
        "nb_preds = ...\n",
        "\n",
        "nbeats_smapes = eval_forecasts(nb_preds, air_test)\n",
        "nbeats_elapsed_time = time.time() - start_time"
      ],
      "metadata": {
        "id": "_Yy85Z7tsoKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smapes_2 = {**smapes,\n",
        "    **{\n",
        "    # ... Fill in here sMAPEs values of any global model you tried\n",
        "    'NBeats': nbeats_smapes,\n",
        "    }\n",
        "}\n",
        "smapes_boxplot(smapes_2, title='sMAPEs on air')"
      ],
      "metadata": {
        "id": "WWipgclvfKel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elapsed_time_2 = {**elapsed_times,\n",
        "    **{\n",
        "    # ... Fill in here duration values of any global model you tried\n",
        "    'NBeats': nbeats_elapsed_time,\n",
        "    }\n",
        "}\n",
        "elapsed_time_barplot(elapsed_time_2, title='Durations on air')"
      ],
      "metadata": {
        "id": "xa9yzSKTgxSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are your conclusions so far, and which results did you manage to get (let us know!)\n",
        "\n",
        "## Part 3: Training an N-BEATS model on `m4` dataset and use it to forecast `air` dataset\n",
        "Deep learning models often do better when trained on *large* datasets. Let's try to load all 48,000 monthly time series in the M4 dataset and train our model once more on this larger dataset."
      ],
      "metadata": {
        "id": "vCzqR2xQzrAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m4_train, m4_test = load_m4()\n",
        "\n",
        "# filter to keep only those that are long enough\n",
        "m4_train = [s for s in m4_train if len(s) >= 48]\n",
        "m4_test = [s for s in m4_test if len(s) >= 48]\n",
        "\n",
        "print('There are {} series of length >= 48.'.format(len(m4_train)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn3p5nat1aip",
        "outputId": "ed0140b0-a7fc-4baa-b81a-6fbc37020459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading M4 TimeSeries...\n",
            "done. There are 48000 series, with average training length 216.30022916666667\n",
            "There are 47992 series of length >= 48.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can start from the same hyper-parameters as before. \n",
        "\n",
        "With 48,000 M4 training series being on average ~200 time steps long, we would end up with ~10M training samples. With such a number of training samples, each epoch would take too long. So here, we'll limit the number of training samples used per series. This is done when calling `fit()` with the parameter `max_samples_per_ts`. We add a new hyper-parameter `MAX_SAMPLES_PER_TS` to capture this.\n",
        "\n",
        "Since the M4 training series are all >= 48 time steps long, we can also use a longer `input_chunk_length` of 36."
      ],
      "metadata": {
        "id": "rZ84E__e1vfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Slicing hyper-params:\n",
        "IN_LEN = 36\n",
        "OUT_LEN = 12\n",
        "\n",
        "# Architecture hyper-params:\n",
        "NUM_STACKS = 18\n",
        "NUM_BLOCKS = 3\n",
        "NUM_LAYERS = 3\n",
        "LAYER_WIDTH = 180\n",
        "COEFFS_DIM = 6\n",
        "\n",
        "# Training settings:\n",
        "LR = 5e-4\n",
        "BATCH_SIZE = 1024\n",
        "MAX_SAMPLES_PER_TS = 8   # <-- new param, limiting nr of training samples per epoch\n",
        "NUM_EPOCHS = 4"
      ],
      "metadata": {
        "id": "-zyOtrjGreEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now build and train the model, as before.\n",
        "\n",
        "Running this cell with the proposed hyper-parameters should take ~10 minutes on a Colab GPU. \n",
        "\n",
        "*If this is taking too long, you can also simply run the next cell, which will download and load the same N-BEATS pre-trained on M4 with these hyper-parameters.*"
      ],
      "metadata": {
        "id": "Wnc0WLVTC2tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "nbeats_model_m4 = NBEATSModel(..., # fill in hyper-params\n",
        "                              \n",
        "                              # learning rate goes here\n",
        "                              optimizer_kwargs={'lr': LR},\n",
        "\n",
        "                              # remove this one if your notebook does not have a GPU:\n",
        "                              pl_trainer_kwargs={\"enable_progress_bar\": True, \n",
        "                                                 \"accelerator\": \"gpu\",\n",
        "                                                 \"gpus\": -1,\n",
        "                                                 \"auto_select_gpus\": True},\n",
        "                              )\n",
        "\n",
        "# Train\n",
        "nbeats_model_m4.fit(..., # fill in series to train on\n",
        "                    ..., # fill in number of epochs\n",
        "                    ...) # fill in max number of samples per time series"
      ],
      "metadata": {
        "id": "MRgxs08greJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below will download a pre-trained version of this N-BEATS model - you can run this if training takes too long in your case:"
      ],
      "metadata": {
        "id": "HozcYBjwD5Vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## /!\\ RUNNING THIS CELL WILL DOWNLOAD AND OVERWRITE THE MODEL nbeats_model_m4\n",
        "\n",
        "# Load already trained model\n",
        "!curl -L https://github.com/unit8co/amld2022-forecasting-and-metalearning/blob/main/data/nbeats_model_m4.pth.tar\\?raw\\=true -o nbeats_model_m4.pth.tar\n",
        "nbeats_model_m4 = NBEATSModel.load_model('nbeats_model_m4.pth.tar')"
      ],
      "metadata": {
        "id": "72jubSWvqxoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use our M4-trained model to get forecasts for the air passengers series. As we use the model in a \"meta learning\" (or transfer learning) way here, we will be timing only the inference part."
      ],
      "metadata": {
        "id": "5_6yhZAx5uy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "preds = ... # get forecasts\n",
        "nbeats_m4_smapes = eval_forecasts(preds, air_test)\n",
        "nbeats_m4_elapsed_time = time.time() - start_time"
      ],
      "metadata": {
        "id": "1S2esxXzreNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are your conclusions?\n",
        "\n",
        "### Try training other global models on `m4` and applying on airline passengers\n",
        "You can now try to train other global models on the M4 dataset in order to see if we can get similar results. If that's taking too long, it might be a good idea to take only e.g., 5000 or 10000 time series. You can do this easily by training on, say, `random.choices(m4_train, k=5000)` instead of `m4_train`. You will again need to specify some small enough value for `max_samples_per_ts` in order to limit the number of training samples."
      ],
      "metadata": {
        "id": "sV2PDSrkpVXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_X = GlobalModelX(...)\n",
        "# model_X.fit(...,\n",
        "#             max_samples_per_ts=...)"
      ],
      "metadata": {
        "id": "dPrjayclreQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "# preds = ...  # Get predictions\n",
        "# model_X_smapes = eval_forecasts(preds, air_test)  # compute errors\n",
        "# model_X_elapsed_time = time.time() - start_time  # store timing"
      ],
      "metadata": {
        "id": "HosI0UZKpwSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now compare how our different models are doing"
      ],
      "metadata": {
        "id": "2ggBmfigLkuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smapes_3 = {**smapes_2,\n",
        "    **{\n",
        "    'N-BEATS M4': nbeats_m4_smapes,\n",
        "    # 'model_X': model_X_smapes\n",
        "    }\n",
        "}\n",
        "smapes_boxplot(smapes_3, title='sMAPEs on air')\n",
        "\n",
        "elapsed_time_3 = {**elapsed_time_2,\n",
        "    **{\n",
        "    'NBeats M4': nbeats_m4_elapsed_time,\n",
        "    # 'model_X': model_X_elapsed_time\n",
        "    }\n",
        "}\n",
        "elapsed_time_barplot(elapsed_time_3, title='Durations on air')"
      ],
      "metadata": {
        "id": "mVFumYDjiGEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Forecasting the `m3` dataset\n",
        "\n",
        "Until now, we have seen that we can use some M4-trained models to predict another dataset, namely the `air` dataset.\n",
        "But can we try to convince ourselves a bit more, and try the same approach on a third dataset?\n",
        "\n",
        "In this part of the notebook, we propose to consolidate all our learnings so far using the `m3` dataset:\n",
        "* Try fitting local models directly on `m3`\n",
        "* Try fitting global ML models directly on `m3` --> how far can you push it?\n",
        "* Try applying our previous M4-trained model on `m3` --> what are your conclusions?\n",
        "\n",
        "Hint: The Theta model was one of the best performing model during the M3 competition."
      ],
      "metadata": {
        "id": "gtkyiQ_OvUNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, load the actual dataset\n",
        "m3_train, m3_test = load_m3()"
      ],
      "metadata": {
        "id": "WGQWwvyHpTaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then try your models :)"
      ],
      "metadata": {
        "id": "R44ESec7o0sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BN9Q3tEGCte0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "amld2022-forecasting-meta-learning",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}